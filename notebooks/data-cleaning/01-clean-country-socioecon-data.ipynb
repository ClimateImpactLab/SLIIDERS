{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08e3612",
   "metadata": {},
   "source": [
    "## Preparing and cleaning files necessary for (country-level) capital stock projection workflow\n",
    "\n",
    "## Importing necessary modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9622a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7606bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import dask_geopandas as dgp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyreadr\n",
    "import rioxarray\n",
    "import sliiders.settings as sset\n",
    "import xarray as xr\n",
    "\n",
    "from PyPDF2 import PdfFileReader\n",
    "from sliiders.cia_wfb_clean import organize_gather_cia_wfb_2000_2022\n",
    "from sliiders.country_level_ypk import ppp_conversion_specific_year\n",
    "from sliiders.io import read_shapefile, save, save_geoparquet\n",
    "from sliiders.spatial import grid_val_to_ix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530700ee-d556-45c8-a254-1333a3a459c7",
   "metadata": {},
   "source": [
    "## Clean GADM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd22804b-711e-4b79-97f9-85661e57db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "adm0 = read_shapefile(sset.PATH_GADM, layer=0).set_crs(epsg=4326)\n",
    "\n",
    "# handle autonomous regions which are given separate iso3 code but we will consider part\n",
    "# of associated sovereign\n",
    "autonomous = adm0[\n",
    "    adm0.GID_0.str.startswith(\"Z\") & adm0.GID_0.str[1].isin(list(\"1234567890\"))\n",
    "]\n",
    "autonomous = (\n",
    "    autonomous.join(\n",
    "        pd.read_parquet(sset.PATH_HIST_CCODE_MAPPING).ccode, on=\"COUNTRY\", how=\"left\"\n",
    "    )\n",
    "    .drop(columns=\"COUNTRY\")\n",
    "    .rename(columns={\"ccode\": \"GID_0\", \"GID_0\": \"parent\"})\n",
    ")\n",
    "adm0 = adm0.drop(columns=\"COUNTRY\")\n",
    "full = pd.concat(\n",
    "    (\n",
    "        adm0.loc[adm0.GID_0.isin(autonomous.GID_0.unique())],\n",
    "        autonomous.drop(columns=\"parent\"),\n",
    "    )\n",
    ").dissolve(by=\"GID_0\")\n",
    "adm0 = pd.concat(\n",
    "    (adm0.set_index(\"GID_0\").drop(full.index.union(autonomous.parent)), full)\n",
    ").geometry.sort_index()\n",
    "assert adm0.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dab4567-1883-4687-9303-9a9c4b831ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1 = read_shapefile(sset.PATH_GADM, layer=1)[[\"GID_0\", \"GID_1\", \"geometry\"]].set_crs(\n",
    "    epsg=4326\n",
    ")\n",
    "\n",
    "# manual fixes\n",
    "\n",
    "# drop a NA country\n",
    "adm1 = adm1[adm1.GID_0.ne(\"NA\")]\n",
    "\n",
    "# fix Ukraine GID_1 with \"?\" as name\n",
    "adm1.loc[adm1.GID_0.eq(\"UKR\") & adm1.GID_1.eq(\"?\"), \"GID_1\"] = \"UKR.28_1\"\n",
    "\n",
    "# Fix Ghana adm1 names which don't have \".\"\n",
    "adm1.loc[adm1.GID_0.eq(\"GHA\"), \"GID_1\"] = (\n",
    "    \"GHA.\" + adm1.loc[adm1.GID_0.eq(\"GHA\"), \"GID_1\"].str[3:]\n",
    ")\n",
    "\n",
    "mapper = autonomous.set_index(\"parent\").GID_0.to_dict()\n",
    "adm1[\"GID_0\"] = adm1.GID_0.replace(mapper)\n",
    "parent = adm1.GID_1.str.split(\".\").str\n",
    "adm1[\"GID_1\"] = pd.Series(parent[0]).replace(mapper).values + (\".\" + parent[1])\n",
    "adm1 = adm1.set_index(\"GID_1\")\n",
    "dups = adm1[adm1.index.duplicated(keep=False)]\n",
    "agg = dups.reset_index().dissolve(by=\"GID_1\")\n",
    "adm1 = pd.concat((adm1.drop(agg.index), agg)).sort_index()\n",
    "assert adm1.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "639f221c-2fce-4e0e-a2d2-09284924d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are treating HKG and MAC as their own adm0's, which we must manually pull out from\n",
    "# GADM adm0\n",
    "adm0 = pd.concat(\n",
    "    (\n",
    "        adm0,\n",
    "        adm1.loc[[\"CHN.HKG\", \"CHN.MAC\"]]\n",
    "        .set_index(pd.Index([\"HKG\", \"MAC\"], name=\"GID_0\"))\n",
    "        .geometry,\n",
    "    )\n",
    ").sort_index()\n",
    "adm1 = adm1.drop([\"CHN.HKG\", \"CHN.MAC\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cb7f207-c089-49fb-a8b7-f0e28916e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/git-repos/sliiders/sliiders/io.py:74: UserWarning: this is an initial implementation of Parquet/Feather file support and associated metadata.  This is tracking version 0.1.0 of the metadata specification at https://github.com/geopandas/geo-arrow-spec\n",
      "\n",
      "This metadata specification does not yet make stability promises.  We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files.\n",
      "\n",
      "To further ignore this warning, you can do: \n",
      "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
      "  obj.to_parquet(_path, **kwargs)\n",
      "/home/jovyan/git-repos/sliiders/sliiders/io.py:74: UserWarning: this is an initial implementation of Parquet/Feather file support and associated metadata.  This is tracking version 0.1.0 of the metadata specification at https://github.com/geopandas/geo-arrow-spec\n",
      "\n",
      "This metadata specification does not yet make stability promises.  We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files.\n",
      "\n",
      "To further ignore this warning, you can do: \n",
      "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
      "  obj.to_parquet(_path, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "save_geoparquet(adm0.to_frame(\"geometry\"), sset.PATH_GADM_ADM0_INT)\n",
    "save_geoparquet(adm1, sset.PATH_GADM_ADM1_INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207b1e9-62fd-43b1-8265-a923d4ab8eb7",
   "metadata": {},
   "source": [
    "## Get country codes from GADM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "994246ad-0616-43c7-a4b8-675c2a2e1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccode_mapping = (\n",
    "    read_shapefile(sset.PATH_GADM, layer=0)\n",
    "    .set_index(\"COUNTRY\")\n",
    "    .GID_0.rename(\"ccode\")\n",
    "    .rename_axis(\"name\")\n",
    ")\n",
    "# drop numerical china/india/pakistan GID_0's\n",
    "ccode_mapping = ccode_mapping[ccode_mapping.str[1] != \"0\"]\n",
    "\n",
    "# add on manaully added segments (which account for uninhabited areas not in GADM)\n",
    "ccode_mapping = pd.concat(\n",
    "    (ccode_mapping, pd.read_parquet(sset.PATH_SEG_PTS_MANUAL).ccode)\n",
    ")\n",
    "\n",
    "# add some manual mappers\n",
    "# Netherlands Antilles in CIA WFB corresponds to these three (not ABW)\n",
    "manual = sset.CCODE_MANUAL.copy()\n",
    "manual[\"Netherlands Antilles\"] = \"BES+CUW+SXM\"\n",
    "ccode_mapping = pd.concat([ccode_mapping, manual]).sort_index()\n",
    "\n",
    "# Handle no-accent names\n",
    "alt_index = (\n",
    "    ccode_mapping.index.str.normalize(\"NFKD\")\n",
    "    .str.encode(\"ascii\", errors=\"ignore\")\n",
    "    .astype(str)\n",
    ")\n",
    "alt = pd.Series(ccode_mapping.values, index=alt_index, name=\"ccode\")\n",
    "ccode_mapping = (\n",
    "    pd.concat((ccode_mapping, alt))\n",
    "    .reset_index()\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"name\")\n",
    "    .ccode.sort_index()\n",
    ")\n",
    "\n",
    "# getting list of valid ccodes including some previously uncaptured mixtures (i.e.\n",
    "# France + overseas depts)\n",
    "valid_ccodes = np.setdiff1d(\n",
    "    np.unique(\n",
    "        np.concatenate(\n",
    "            (\n",
    "                ccode_mapping.unique(),\n",
    "                [k for v in sset.PPP_CCODE_IF_MSNG.values() for k in v],\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    sset.EXCLUDED_ISOS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0525373-d81a-4710-b729-de857effa1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(pd.DataFrame({\"ccode\": valid_ccodes}), sset.PATH_ALL_VALID_HIST_CCODES)\n",
    "save(ccode_mapping.to_frame(), sset.PATH_HIST_CCODE_MAPPING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b195b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## UN WPP: overall populations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a9ebc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_df = []\n",
    "for ix, sex in enumerate([\"MALE\", \"FEMALE\"]):\n",
    "    df = pd.read_excel(\n",
    "        sset.DIR_UN_WPP_RAW\n",
    "        / f\"WPP2022_POP_F02_{ix+2}_POPULATION_5-YEAR_AGE_GROUPS_{sex}.xlsx\",\n",
    "        sheet_name=\"Estimates\",\n",
    "        skiprows=16,\n",
    "        index_col=0,\n",
    "    )\n",
    "    un_df.append(\n",
    "        df[df.Type.eq(\"Country/Area\")]\n",
    "        .drop(\n",
    "            columns=[\n",
    "                \"Variant\",\n",
    "                \"Region, subregion, country or area *\",\n",
    "                \"Notes\",\n",
    "                \"Location code\",\n",
    "                \"ISO2 Alpha-code\",\n",
    "                \"Type\",\n",
    "                \"SDMX code**\",\n",
    "                \"Parent code\",\n",
    "            ]\n",
    "        )\n",
    "        .rename(columns={\"ISO3 Alpha-code\": \"ccode\", \"Year\": \"year\"})\n",
    "        .astype({\"year\": int})\n",
    "        .assign(sex=sex.lower())\n",
    "    )\n",
    "\n",
    "# rename kosovo to match gadm\n",
    "un_df = (\n",
    "    pd.concat(un_df)\n",
    "    .replace({\"ccode\": {\"XKX\": \"XKO\"}})\n",
    "    .set_index([\"ccode\", \"year\", \"sex\"])\n",
    ")\n",
    "\n",
    "# pop is in thousands\n",
    "un_df *= 1000\n",
    "\n",
    "# make sure all ccodes match\n",
    "un_ccodes = un_df.index.get_level_values(\"ccode\").unique()\n",
    "assert un_ccodes.isin(valid_ccodes).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6c9b2a36-21b9-46e2-8b4a-f1f324cf58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(un_df, sset.PATH_UN_WPP_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "550125c6-c6ba-452f-8d8e-2d81fa62cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del un_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857d146-7adc-4630-8736-e55c51bd8c20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GEG-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d730e8c-708e-49fc-a273-7483c9556422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geg = dgp.read_file(sset.PATH_GEG15_RAW, npartitions=8)\n",
    "df_geg = (\n",
    "    df_geg.assign(lon=df_geg.geometry.x, lat=df_geg.geometry.y)\n",
    "    .drop(columns=\"geometry\")[[\"iso3\", \"tot_val\", \"lon\", \"lat\"]]\n",
    "    .compute()\n",
    "    .set_index([\"lon\", \"lat\", \"iso3\"])\n",
    ")\n",
    "assert np.isin(df_geg.index.get_level_values(\"iso3\").unique(), valid_ccodes).all()\n",
    "\n",
    "# convert from millions to ones\n",
    "df_geg[\"tot_val\"] *= 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "918375f6-f8f4-4e58-a10f-b0ee637b4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df_geg, sset.PATH_GEG15_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1608f3a6-d577-4b11-a389-b705aa1e1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_geg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b1898d-3567-4c24-a08c-06f39f03ae58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Landscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914d1351-1a2a-483e-86b5-ca5160364ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rioxarray.open_rasterio(\n",
    "    sset.PATH_LANDSCAN_RAW, mask_and_scale=True, chunks=\"auto\"\n",
    ").squeeze()\n",
    "df = df.where(df > 0).to_series().dropna().rename(\"population\").to_frame()\n",
    "df[[\"x_ix\", \"y_ix\"]] = grid_val_to_ix(\n",
    "    df.reset_index()[[\"x\", \"y\"]].values,\n",
    "    sset.POP_GRID_WIDTH,\n",
    "    lon_mask=[True, False],\n",
    ")\n",
    "df = df.set_index([\"x_ix\", \"y_ix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb6d6453-af97-4975-b9e9-5d92fa3437ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\n",
    "    str(sset.PATH_EXPOSURE_POP_INT), storage_options=sset.STORAGE_OPTIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fb23914-6f38-40c7-837f-1b4a43130e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2ac64",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CIA World Factbook organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e09964-6faf-4964-9b0d-e92b2fa95cf5",
   "metadata": {},
   "source": [
    "Here, the following are carried out:\n",
    "1. Clean each yearly version into `pandas.DataFrame` format\n",
    "2. Attach ISO-3166 alpha-3 codes for easier merging\n",
    "3. Merge the different versions into one dataset; update older data with newer data whenever possible\n",
    "4. For GDP and GDP per capita, make sure that they are in constant 2017 PPP USD terms, as the raw dataset has varying PPP USD years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661eac3-4143-45b1-8755-32ffd28660a9",
   "metadata": {},
   "source": [
    "### Clean the yearly versions and attach country codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bab6dd-383a-4059-9f69-ff081521ca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029b8f562f6544719ad093f8b55e2666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cia_wfb_pop, cia_wfb_gdp, cia_wfb_gdppc = organize_gather_cia_wfb_2000_2022(\n",
    "    sset.DIR_CIA_RAW, ccode_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862b2e1-e992-4fa8-93bb-5cc0af40cf2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manual pop additions (too difficult to parse html)\n",
    "\n",
    "# Akrotiri and Dhekelia\n",
    "xad_df = pd.Series(\n",
    "    [8500, 15700, 18195],\n",
    "    index=pd.Index([2004, 2008, 2020], name=\"year\", dtype=\"uint64\"),\n",
    "    name=\"pop\",\n",
    ")\n",
    "\n",
    "# British Indian Ocean Territories\n",
    "iot_df = pd.Series(\n",
    "    [1200, 3200, 2500, 4000, 3000, 3000],\n",
    "    index=pd.Index([1960, 1995, 2001, 2004, 2014, 2020], name=\"year\", dtype=\"uint64\"),\n",
    "    name=\"pop\",\n",
    ")\n",
    "extra_pops = (\n",
    "    pd.concat(\n",
    "        [xad_df.to_frame().assign(ccode=\"XAD\"), iot_df.to_frame().assign(ccode=\"IOT\")]\n",
    "    )\n",
    "    .assign(wfb_year=9999)\n",
    "    .reset_index()\n",
    "    .set_index([\"ccode\", \"year\"])\n",
    ")\n",
    "\n",
    "assert not cia_wfb_pop.index.get_level_values(\"ccode\").isin([\"XAD\", \"IOT\"]).any()\n",
    "cia_wfb_pop = pd.concat([cia_wfb_pop, extra_pops]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb9d93-c1f7-4a59-901e-68a88aeddc30",
   "metadata": {},
   "source": [
    "### Turning into constant 2017 PPP USD terms for GDP and GDP per capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde456f9-ca2a-4100-b0b7-4fd9ae372e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetching the PPP conversion factors\n",
    "ppp_to_17 = ppp_conversion_specific_year(\n",
    "    2017,\n",
    "    sset.PATH_PWT_RAW,\n",
    "    to=True,\n",
    "    extrap_sim=True,\n",
    "    fill_msng_ctries=sset.PPP_CCODE_IF_MSNG,\n",
    ")\n",
    "\n",
    "# checking the country codes that are not in `ppp_to_17`\n",
    "print(\n",
    "    \"Missing from the PPP conversion table:\\n\",\n",
    "    cia_wfb_gdp.index.get_level_values(\"ccode\")\n",
    "    .union(cia_wfb_gdppc.index.get_level_values(\"ccode\"))\n",
    "    .difference(ppp_to_17.index.get_level_values(\"ccode\")),\n",
    ")\n",
    "\n",
    "# changing the 'year' index to be named 'usd_year'\n",
    "ppp_to_17 = (\n",
    "    ppp_to_17.reset_index()\n",
    "    .rename(columns={\"year\": \"usd_year\"})\n",
    "    .set_index([\"ccode\", \"usd_year\"])\n",
    ")\n",
    "\n",
    "\n",
    "# extrapolate to 2020\n",
    "ppp_xr = ppp_to_17.to_xarray()\n",
    "ppp_xr_add = ppp_xr.sel(usd_year=2019, drop=True).expand_dims(usd_year=[2020])\n",
    "\n",
    "fill_val = xr.DataArray(\"2019 value held fixed to 2021\").broadcast_like(ppp_xr_add)\n",
    "ppp_xr_add[\"conv_fill\"] = fill_val\n",
    "ppp_xr_add[\"pl_gdpo_fill\"] = fill_val\n",
    "ppp_to_17 = xr.concat((ppp_xr, ppp_xr_add), dim=\"usd_year\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b1e70-8882-4193-8bac-0ca71849cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching the USD GDP deflators\n",
    "defla_to_17 = (\n",
    "    pd.read_excel(sset.PATH_PWT_RAW)\n",
    "    .rename(columns={\"year\": \"usd_year\"})\n",
    "    .set_index([\"countrycode\", \"usd_year\"])\n",
    ")\n",
    "defla_to_17 = (\n",
    "    defla_to_17.loc[([\"USA\"], slice(None)), [\"pl_gdpo\"]]\n",
    "    .reset_index()\n",
    "    .drop([\"countrycode\"], axis=1)\n",
    "    .set_index([\"usd_year\"])\n",
    ")\n",
    "defla_to_17[\"gdp_defla\"] = defla_to_17.loc[2017, \"pl_gdpo\"] / defla_to_17[\"pl_gdpo\"]\n",
    "defla_to_17.drop([\"pl_gdpo\"], axis=1, inplace=True)\n",
    "\n",
    "# PWT ends in 2019. We add 2020 and 2021 based on WB USA deflation data\n",
    "# (https://data.worldbank.org/indicator/NY.GDP.DEFL.KD.ZG?locations=US)\n",
    "defla_to_17 = pd.concat(\n",
    "    (\n",
    "        defla_to_17,\n",
    "        defla_to_17.loc[2019, \"gdp_defla\"]\n",
    "        * pd.DataFrame(\n",
    "            {\"gdp_defla\": [0.988]},\n",
    "            index=pd.Index([2020], name=\"usd_year\"),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# merging with the PPP conversion rates\n",
    "ppp_to_17 = ppp_to_17.merge(defla_to_17, left_index=True, right_index=True, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d53658-93dc-498e-8f24-724e16f3a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we manually check if USD year terms agree with one another; if they don't, we check\n",
    "# the WFB versions and use the available USD years (some are assumed from their years)\n",
    "check_usd_year = cia_wfb_gdppc.merge(\n",
    "    cia_wfb_gdp, how=\"outer\", left_index=True, right_index=True\n",
    ")\n",
    "check_usd_year = check_usd_year.loc[\n",
    "    (check_usd_year.usd_year_y != check_usd_year.usd_year_x)\n",
    "    & ~pd.isnull(check_usd_year.usd_year_y)\n",
    "    & ~pd.isnull(check_usd_year.usd_year_x)\n",
    "]\n",
    "\n",
    "mismatched_usd_year = np.unique(check_usd_year.index.get_level_values(\"ccode\"))\n",
    "print(\n",
    "    \"Manually check the following countries:\\n\",\n",
    "    mismatched_usd_year,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4160a53-80dc-4a93-887b-c84ce510c3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manual cleansing for USD years\n",
    "take_usd_year_from_gdp = [\n",
    "    (\"AND\", [2010, 2011, 2013, 2014, 2015]),\n",
    "    (\"ASM\", [2014, 2015]),\n",
    "    (\"GGY\", [2014]),\n",
    "    (\"GNQ\", [2011, 2012]),\n",
    "    (\"GRL\", [2013, 2014]),\n",
    "    (\"JEY\", [2015]),\n",
    "    (\"MAC\", [2006, 2008, 2014, 2016]),\n",
    "    (\"MCO\", [2006, 2009, 2011, 2013, 2014]),\n",
    "    (\"MHL\", [2008]),\n",
    "    (\"MNP\", [2014, 2015, 2016]),\n",
    "    (\"PLW\", [2008]),\n",
    "    (\"PSE\", [2012, 2013]),\n",
    "    (\"SOM\", [2013, 2009, 2008]),\n",
    "    (\"SSD\", [2010]),\n",
    "    (\"TUV\", [2010]),\n",
    "    (\"VIR\", [2011, 2012, 2014, 2015, 2016]),\n",
    "]\n",
    "\n",
    "take_usd_year_from_gdppc = [\n",
    "    ([\"FSM\", \"NRU\", \"PLW\"], 2013),\n",
    "]\n",
    "\n",
    "for i in take_usd_year_from_gdp:\n",
    "    cia_wfb_gdppc.loc[i, \"usd_year\"] = cia_wfb_gdp.loc[i, \"usd_year\"]\n",
    "\n",
    "for i in take_usd_year_from_gdppc:\n",
    "    cia_wfb_gdp.loc[i, \"usd_year\"] = cia_wfb_gdppc.loc[i, \"usd_year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa3c92-6cfc-4b5a-a222-068cd2e7a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_dollars(df, name):\n",
    "\n",
    "    alt_names = {\"gdp\": \"rgdpna_17\", \"gdppc\": \"rgdpna_pc_17\"}\n",
    "    # GDP per capita; not using index merging due to ccode-usd_year indices not being\n",
    "    # unique in CIA WFB datasets\n",
    "    out = df.reset_index().join(ppp_to_17, how=\"left\", on=[\"ccode\", \"usd_year\"])\n",
    "\n",
    "    out.loc[out.conv.isnull(), [\"conv_fill\", \"pl_gdpo_fill\"]] = \"neutral_assumption\"\n",
    "\n",
    "    out.loc[out.conv.isnull(), \"conv\"] = 1\n",
    "\n",
    "    # only turning USD values to 2017 USD values, as we aren't too sure about PPP base year\n",
    "    out[f\"{name}_usd_17\"] = out[[name, \"gdp_defla\"]].product(axis=1)\n",
    "\n",
    "    # assuming PPP year = USD year, turning to constant 2017 PPP USD terms\n",
    "    out[alt_names[name]] = out[[\"conv\", f\"{name}_usd_17\"]].product(axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "ppp_17_gdppc_df = adjust_dollars(cia_wfb_gdppc, \"gdppc\")\n",
    "ppp_17_gdp_df = adjust_dollars(cia_wfb_gdp, \"gdp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84b16a-c9fd-4015-9a94-9e3c38439609",
   "metadata": {},
   "source": [
    "### Merging population, GDP, and GDP per capita datasets altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a7ee0-22a3-4f5a-b079-f605f576e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prep_merge(df, name):\n",
    "    return (\n",
    "        df.rename(\n",
    "            columns={\n",
    "                k: f\"{k}_{name}\" for k in [\"wfb_year\", \"conv_fill\", \"pl_gdpo_fill\"]\n",
    "            }\n",
    "        )\n",
    "        .rename(columns={\"usd_year\": f\"orig_usd_year_{name}\"})\n",
    "        .drop(columns=[\"gdp_defla\", \"conv\", name])\n",
    "        .set_index([\"ccode\", \"year\"])\n",
    "    )\n",
    "\n",
    "\n",
    "gdp_merge_ready = _prep_merge(ppp_17_gdp_df, \"gdp\")\n",
    "gdppc_merge_ready = _prep_merge(ppp_17_gdppc_df, \"gdppc\")\n",
    "\n",
    "all_merged = (\n",
    "    cia_wfb_pop.rename(columns={\"wfb_year\": \"wfb_year_pop\"})\n",
    "    .join(\n",
    "        [gdp_merge_ready, gdppc_merge_ready],\n",
    "        how=\"outer\",\n",
    "    )\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1d048-7c46-4b6b-95e5-6d3af4986d22",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a5f66-db46-4f99-9cb2-8b3743709271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-ordering and changing data types for cleaner viewing\n",
    "ordering = [\n",
    "    \"pop\",\n",
    "    \"gdp_usd_17\",\n",
    "    \"rgdpna_17\",\n",
    "    \"gdppc_usd_17\",\n",
    "    \"rgdpna_pc_17\",\n",
    "    \"wfb_year_pop\",\n",
    "    \"wfb_year_gdp\",\n",
    "    \"wfb_year_gdppc\",\n",
    "    \"orig_usd_year_gdp\",\n",
    "    \"orig_usd_year_gdppc\",\n",
    "    \"conv_fill_gdp\",\n",
    "    \"conv_fill_gdppc\",\n",
    "    \"pl_gdpo_fill_gdp\",\n",
    "    \"pl_gdpo_fill_gdppc\",\n",
    "]\n",
    "all_merged = all_merged[ordering]\n",
    "\n",
    "# won't have ppp for years after PWT\n",
    "max_year = ppp_to_17.index.get_level_values(\"usd_year\").max()\n",
    "all_merged = all_merged.loc[(slice(None), slice(None, max_year)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461acae5-3a5b-4e79-9fc9-5c8b2cf733de",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all_merged.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa274e1b-64a2-47d2-a6ab-c2c390d4b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(all_merged, sset.PATH_CIA_INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8731e3-b0dc-47a6-a777-ea37bb979bd9",
   "metadata": {},
   "source": [
    "## Credit Suisse Global Wealth Databook (GWDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e08b16-1945-4120-9547-22944dc94931",
   "metadata": {},
   "outputs": [],
   "source": [
    "GWDB_REG = [\n",
    "    \"Africa\",\n",
    "    \"Asia-Pacific\",\n",
    "    \"China\",\n",
    "    \"Europe\",\n",
    "    \"India\",\n",
    "    \"Latin America\",\n",
    "    \"North America\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9d3ab-8950-43ce-9f76-4a238a6eb436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_region_mapping(page_text):\n",
    "\n",
    "    # manual cleanup\n",
    "    page_text = page_text.replace(\n",
    "        \"St. Vincent and the \\nGrenadines\", \"St. Vincent and the Grenadines\"\n",
    "    )\n",
    "    page_text_info = page_text.split(\"\\n\")\n",
    "    page_text_info = [\n",
    "        i for i in page_text_info if len(i.strip()) and (not i.startswith(\"Source: \"))\n",
    "    ]\n",
    "\n",
    "    st = [ix for ix, i in enumerate(page_text_info) if \"USD bn\" in i and \"%\" in i]\n",
    "    assert len(st) == 1\n",
    "    st = st[0] + 1\n",
    "\n",
    "    out = []\n",
    "    for line in page_text_info[st:]:\n",
    "        c = line.replace(\" -\", \"-\")\n",
    "        if line.startswith(\"Sources:\"):\n",
    "            continue\n",
    "        matches = re.findall(\"|\".join(GWDB_REG), c)\n",
    "        assert len(set(matches)) == 1, line\n",
    "        match = matches[0]\n",
    "        sep = c.rfind(match) - 1\n",
    "        ctry = c[:sep].strip()\n",
    "        out.append((ctry, match))\n",
    "    return pd.DataFrame(out, columns=[\"country\", \"gwdb_region\"])\n",
    "\n",
    "\n",
    "def parse_page(page_text):\n",
    "    year_info = re.findall(\"\\(end.*-.*\\)\", page_text)\n",
    "    assert len(year_info) == 1\n",
    "    year = int(year_info[0][-5:-1])\n",
    "\n",
    "    # manual cleanup\n",
    "    page_text = page_text.replace(\n",
    "        \"St. Vincent and the Grena-\\ndines\", \"St. Vincent and the Grenadines\"\n",
    "    )\n",
    "    page_text_info = page_text.split(\"\\n\")\n",
    "    page_text_info = [\n",
    "        i for i in page_text_info if len(i.strip()) and (not i.startswith(\"Source: \"))\n",
    "    ]\n",
    "\n",
    "    st = [\n",
    "        ix\n",
    "        for ix, i in enumerate(page_text_info)\n",
    "        if \"thousand\" in i and \"USD bn\" in i and \"%\" in i\n",
    "    ]\n",
    "    assert len(st) == 1\n",
    "    st = st[0] + 1\n",
    "\n",
    "    out = []\n",
    "    for line in page_text_info[st:]:\n",
    "        start = re.search(\"[0-9]\", line).start()\n",
    "        c = (\n",
    "            line[:start]\n",
    "            .rstrip()\n",
    "            .replace(\" -\", \"-\")\n",
    "            .replace(\"- \", \"-\")\n",
    "            .replace(\" \", \"_\")\n",
    "        )\n",
    "        end = re.search(\"(\\d)[^\\d]*$\", line).start()\n",
    "        s = line[end + 1 :].strip().replace(\" \", \"_\")\n",
    "        out.append(c + \" \" + line[start : end + 1].replace(\",\", \"\") + \" \" + s)\n",
    "\n",
    "    data = pd.read_csv(\n",
    "        StringIO(\"\\n\".join(out)),\n",
    "        delim_whitespace=True,\n",
    "        names=[\n",
    "            \"country\",\n",
    "            \"n_adults\",\n",
    "            \"pct_adults\",\n",
    "            \"total_wealth\",\n",
    "            \"pct_wealth\",\n",
    "            \"wealth_per_adult\",\n",
    "            \"financial_wealth_per_adult\",\n",
    "            \"nonfinancial_wealth_per_adult\",\n",
    "            \"debt_per_adult\",\n",
    "            \"median_wealth_per_adult\",\n",
    "            \"estimation_method\",\n",
    "        ],\n",
    "    )\n",
    "    data[\"country\"] = data.country.str.replace(\"_\", \" \")\n",
    "    data[\"estimation_method\"] = data.estimation_method.str.replace(\"_\", \" \").str.lower()\n",
    "\n",
    "    # pop is in thousands\n",
    "    data[\"n_adults\"] *= 1000\n",
    "\n",
    "    # wealth is in billions\n",
    "    data[\"total_wealth\"] *= 1e9\n",
    "    data[\"year\"] = year\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ce921-f33b-4013-8da3-f6687f33f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the file\n",
    "GWDB = PdfFileReader(sset.PATH_GWDB_RAW.open(\"rb\"))\n",
    "\n",
    "gwdb_df = []\n",
    "region_mapping = []\n",
    "for lx in tqdm(range(len(GWDB.pages))):\n",
    "    text = GWDB.getPage(lx).extractText()\n",
    "    if (\n",
    "        \"Table2-2:Wealthestimatesbycountry\" in text.replace(\" \", \"\")\n",
    "        and \"Contents\" not in text\n",
    "    ):\n",
    "        gwdb_df.append(parse_page(text))\n",
    "    elif \"Table2-1:Countrydetails\" in text.replace(\" \", \"\") and \"Contents\" not in text:\n",
    "        region_mapping.append(parse_region_mapping(text))\n",
    "\n",
    "region_mapping = pd.concat(region_mapping).set_index(\"country\").gwdb_region\n",
    "\n",
    "# handle duplicate india and china b/c they are regions and countries\n",
    "gwdb_df = pd.concat(gwdb_df).set_index([\"country\", \"year\"])\n",
    "gwdb_df = gwdb_df[~gwdb_df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "gwdb_df = gwdb_df.join(region_mapping, on=\"country\", how=\"left\")\n",
    "\n",
    "# check that we mapped everyone\n",
    "missing_reg = (\n",
    "    gwdb_df[gwdb_df.gwdb_region.isnull()].index.get_level_values(\"country\").unique()\n",
    ")\n",
    "assert np.isin(missing_reg, np.concatenate((region_mapping.unique(), [\"World\"]))).all()\n",
    "\n",
    "gwdb_df = gwdb_df.join(ccode_mapping.rename_axis(\"country\"), how=\"left\").reset_index()\n",
    "assert not len(\n",
    "    gwdb_df.loc[\n",
    "        gwdb_df.ccode.isnull() & ~gwdb_df.country.isin(GWDB_REG + [\"World\"]), \"country\"\n",
    "    ].unique()\n",
    ")\n",
    "\n",
    "# Fill region ccodes with just their name\n",
    "gwdb_df[\"ccode\"] = gwdb_df.ccode.fillna(gwdb_df.country)\n",
    "gwdb_df = gwdb_df.set_index([\"ccode\", \"year\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1847dcd-25ac-4a46-9899-1f8cba4ed0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(gwdb_df, sset.PATH_GWDB_INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677d83b-2afa-4055-a859-bf28efdb8ee0",
   "metadata": {},
   "source": [
    "## Fariss et al. (2022, Journal of Conflict Resolution) GDP, GDPpc, and population dataset\n",
    "\n",
    "We attach 3-digit ISO codes in `sset.ALL_ISOS` instead of the Gleditsch and Ward (GW) country codes that are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e668fb51-ba5d-47a6-a7f9-320178a3ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gleditsch and Ward code to ISO codes for those that do not match with one another\n",
    "GLEDITSCH_WARD_TO_ISO = [\n",
    "    [\"BHM\", \"BHS\"],\n",
    "    [\"AAB\", \"ATG\"],\n",
    "    [\"ALG\", \"DZA\"],\n",
    "    [\"ANG\", \"AGO\"],\n",
    "    [\"AUS\", \"AUT\"],\n",
    "    [\"AUL\", \"AUS\"],\n",
    "    [\"MNG\", \"MNE\"],\n",
    "    [\"SLV\", \"SVN\"],\n",
    "    [\"BAH\", \"BHR\"],\n",
    "    [\"CAM\", \"KHM\"],\n",
    "    [\"BUL\", \"BGR\"],\n",
    "    [\"BUI\", \"BDI\"],\n",
    "    [\"BAR\", \"BRB\"],\n",
    "    [\"BFO\", \"BFA\"],\n",
    "    [\"BHM\", \"BHS\"],\n",
    "    [\"BHU\", \"BTN\"],\n",
    "    [\"BNG\", \"BGD\"],\n",
    "    [\"BOS\", \"BIH\"],\n",
    "    [\"BRU\", \"BRN\"],\n",
    "    [\"CAO\", \"CMR\"],\n",
    "    [\"CAP\", \"CPV\"],\n",
    "    [\"CDI\", \"CIV\"],\n",
    "    [\"CEN\", \"CAF\"],\n",
    "    [\"CHA\", \"TCD\"],\n",
    "    [\"CON\", \"COG\"],\n",
    "    [\"COS\", \"CRI\"],\n",
    "    [\"CRO\", \"HRV\"],\n",
    "    [\"CZR\", \"CZE\"],\n",
    "    [\"DEN\", \"DNK\"],\n",
    "    [\"DRC\", \"COD\"],\n",
    "    [\"DRV\", \"VNM\"],\n",
    "    [\"EQG\", \"GNQ\"],\n",
    "    [\"ETM\", \"TLS\"],\n",
    "    [\"FRN\", \"FRA\"],\n",
    "    [\"GAM\", \"GMB\"],\n",
    "    [\"GFR\", \"DEU\"],\n",
    "    [\"GRG\", \"GEO\"],\n",
    "    [\"GRN\", \"GRD\"],\n",
    "    [\"GUA\", \"GTM\"],\n",
    "    [\"GUI\", \"GIN\"],\n",
    "    [\"HAI\", \"HTI\"],\n",
    "    [\"HON\", \"HND\"],\n",
    "    [\"ICE\", \"ISL\"],\n",
    "    [\"INS\", \"IDN\"],\n",
    "    [\"IRE\", \"IRL\"],\n",
    "    [\"KBI\", \"KIR\"],\n",
    "    [\"KOS\", \"XKO\"],\n",
    "    [\"KUW\", \"KWT\"],\n",
    "    [\"KYR\", \"KGZ\"],\n",
    "    [\"KZK\", \"KAZ\"],\n",
    "    [\"LAT\", \"LVA\"],\n",
    "    [\"LEB\", \"LBN\"],\n",
    "    [\"LES\", \"LSO\"],\n",
    "    [\"LIB\", \"LBY\"],\n",
    "    [\"LIT\", \"LTU\"],\n",
    "    [\"MAA\", \"MRT\"],\n",
    "    [\"MAD\", \"MDV\"],\n",
    "    [\"MAG\", \"MDG\"],\n",
    "    [\"MAL\", \"MYS\"],\n",
    "    [\"MAS\", \"MUS\"],\n",
    "    [\"MAW\", \"MWI\"],\n",
    "    [\"MLD\", \"MDA\"],\n",
    "    [\"MNC\", \"MCO\"],\n",
    "    [\"MON\", \"MNG\"],\n",
    "    [\"MOR\", \"MAR\"],\n",
    "    [\"MSI\", \"MHL\"],\n",
    "    [\"MYA\", \"MMR\"],\n",
    "    [\"MZM\", \"MOZ\"],\n",
    "    [\"NAU\", \"NRU\"],\n",
    "    [\"NEP\", \"NPL\"],\n",
    "    [\"NEW\", \"NZL\"],\n",
    "    [\"NIG\", \"NGA\"],\n",
    "    [\"NIR\", \"NER\"],\n",
    "    [\"NTH\", \"NLD\"],\n",
    "    [\"OMA\", \"OMN\"],\n",
    "    [\"PAL\", \"PLW\"],\n",
    "    [\"PAR\", \"PRY\"],\n",
    "    [\"PHI\", \"PHL\"],\n",
    "    [\"POR\", \"PRT\"],\n",
    "    [\"ROK\", \"KOR\"],\n",
    "    [\"RUM\", \"ROU\"],\n",
    "    [\"SAF\", \"ZAF\"],\n",
    "    [\"SAL\", \"SLV\"],\n",
    "    [\"SER\", \"SRB\"],\n",
    "    [\"SEY\", \"SYC\"],\n",
    "    [\"SIE\", \"SLE\"],\n",
    "    [\"SIN\", \"SGP\"],\n",
    "    [\"SKN\", \"KNA\"],\n",
    "    [\"SLO\", \"SVK\"],\n",
    "    [\"SLU\", \"LCA\"],\n",
    "    [\"SNM\", \"SMR\"],\n",
    "    [\"SOL\", \"SLB\"],\n",
    "    [\"SPN\", \"ESP\"],\n",
    "    [\"SRI\", \"LKA\"],\n",
    "    [\"SUD\", \"SDN\"],\n",
    "    [\"SVG\", \"VCT\"],\n",
    "    [\"SWD\", \"SWE\"],\n",
    "    [\"TAJ\", \"TJK\"],\n",
    "    [\"TAW\", \"TWN\"],\n",
    "    [\"TAZ\", \"TZA\"],\n",
    "    [\"THI\", \"THA\"],\n",
    "    [\"TOG\", \"TGO\"],\n",
    "    [\"TRI\", \"TTO\"],\n",
    "    [\"UAE\", \"ARE\"],\n",
    "    [\"UKG\", \"GBR\"],\n",
    "    [\"URU\", \"URY\"],\n",
    "    [\"VAN\", \"VUT\"],\n",
    "    [\"ZAM\", \"ZMB\"],\n",
    "    [\"ZIM\", \"ZWE\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c599205c-5464-4ab0-8e45-c9bbfc9827da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing GW code data table, and attaching ISO codes\n",
    "gwstates = (\n",
    "    pyreadr.read_r(sset.DIR_YPK_RAW / \"gwstates.rda\")[\"gwstates\"]\n",
    "    .rename(columns={\"gwcode\": \"gwno\", \"country_name\": \"country\"})[\n",
    "        [\"gwno\", \"country\", \"gwc\", \"microstate\"]\n",
    "    ]\n",
    "    .merge(\n",
    "        pd.DataFrame(GLEDITSCH_WARD_TO_ISO, columns=[\"gwc\", \"ccode\"]),\n",
    "        on=[\"gwc\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .drop_duplicates()\n",
    ")\n",
    "gwstates.loc[pd.isnull(gwstates.ccode), \"ccode\"] = gwstates.loc[\n",
    "    pd.isnull(gwstates.ccode), \"gwc\"\n",
    "].values\n",
    "\n",
    "# unzipping the raw file, and attaching the necessary ISO codes\n",
    "PATH_FARISS = sset.DIR_YPK_RAW / \"Fariss_JCR_2022.zip\"\n",
    "with TemporaryDirectory() as tl:\n",
    "    temp_loc = Path(tl)\n",
    "    with ZipFile(sset.PATH_FARISS, mode=\"r\") as z:\n",
    "        z.extractall(temp_loc)\n",
    "\n",
    "    # sorted, so GDP, GDPpc, and population\n",
    "    filenames = [\"fariss_gdp.parquet\", \"fariss_gdppc.parquet\", \"fariss_pop.parquet\"]\n",
    "    templst = np.sort(list(temp_loc.glob(\"*\")))\n",
    "    for j, file in enumerate(templst):\n",
    "        fname = filenames[j]\n",
    "        file = pyreadr.read_r(file)[None].merge(\n",
    "            gwstates[[\"gwno\", \"ccode\"]], on=\"gwno\", how=\"left\"\n",
    "        )\n",
    "        # making sure the country codes line up with PWT\n",
    "        file.loc[file.ccode == \"FRA\", \"ccode\"] = sset.FRA_OVERSEAS_DEPT\n",
    "        file.loc[(file.ccode == \"CYP\") & (file.year <= 1973), \"ccode\"] = \"CYP+ZNC\"\n",
    "        file.loc[(file.ccode == \"SRB\") & (file.year <= 1999), \"ccode\"] = \"SRB+XKO\"\n",
    "        file = (\n",
    "            file.loc[~pd.isnull(file.ccode), :]\n",
    "            .set_index([\"indicator\", \"ccode\", \"year\"])\n",
    "            .sort_index()\n",
    "        )\n",
    "        save(file, sset.DIR_FARISS_INT / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c3729-f7af-4ae2-b0dc-c993dc1f85d4",
   "metadata": {},
   "source": [
    "## UN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc31783e-95a2-4c95-9c91-19570d9971f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GDP data\n",
    "un_ypc = (\n",
    "    pd.read_csv(sset.DIR_UN_AMA_RAW / \"un_snaama_nom_gdppc.csv\", na_values=\"...\")\n",
    "    .drop(columns=\"Unit\")\n",
    "    .dropna(how=\"any\")\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"Country/Area\": \"name\",\n",
    "            \"Year\": \"year\",\n",
    "            \"GDP, Per Capita GDP - US Dollars\": \"gdppc_nom_current\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "un_y = (\n",
    "    pd.read_csv(sset.DIR_UN_AMA_RAW / \"un_snaama_nom_gdp.csv\", na_values=\"...\")\n",
    "    .drop(columns=\"Unit\")\n",
    "    .dropna(how=\"any\")\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"Country/Area\": \"name\",\n",
    "            \"Year\": \"year\",\n",
    "            \"GDP, at current prices - US Dollars\": \"gdp_nom_current\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "un_y = un_y.join(un_ypc.iloc[:, -1])\n",
    "\n",
    "# adjust for tanzania being split between mainland and zanzibar\n",
    "tanzania = (\n",
    "    un_y[un_y.name.str.contains(\"Tanzania\")].groupby([\"year\"], as_index=False).sum()\n",
    ")\n",
    "tanzania[\"name\"] = \"United Republic of Tanzania\"\n",
    "un_y = pd.concat((un_y[~un_y[\"name\"].str.contains(\"Tanzania\")], tanzania))\n",
    "\n",
    "# load mapping of country to (sub)region\n",
    "un_regions = pd.read_csv(\n",
    "    sset.PATH_UN_REGION_DATA_RAW,\n",
    "    sep=\";\",\n",
    "    usecols=[\"Country or Area\", \"ISO-alpha3 Code\", \"Sub-region Name\", \"Region Name\"],\n",
    ").rename(\n",
    "    columns={\n",
    "        \"Country or Area\": \"name\",\n",
    "        \"ISO-alpha3 Code\": \"ccode\",\n",
    "        \"Sub-region Name\": \"subregion\",\n",
    "        \"Region Name\": \"region\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# add a few old countries manually to match with gdp dataset\n",
    "manual = [\n",
    "    (\n",
    "        \"Former Netherlands Antilles\",\n",
    "        \"Latin America and the Caribbean\",\n",
    "        \"Americas\",\n",
    "        \"BES+CUW+SXM\",\n",
    "    ),\n",
    "    (\"Former Sudan\", \"Sub-Saharan Africa\", \"Africa\", \"SDN+SSD\"),\n",
    "    (\"Kosovo\", \"Eastern Europe\", \"Europe\", \"XKO\"),\n",
    "    (\"Cyprus/Northern Cyprus\", \"Western Asia\", \"Asia\", \"CYP+ZNC\"),\n",
    "    (\"Akrotiri and Dhekelia\", \"Western Asia\", \"Asia\", \"XAD\"),\n",
    "    (\"Paracel Islands\", \"Eastern Asia\", \"Asia\", \"XPI\"),\n",
    "]\n",
    "un_regions = pd.concat(\n",
    "    (un_regions, pd.DataFrame(manual, columns=[\"name\", \"subregion\", \"region\", \"ccode\"]))\n",
    ")\n",
    "\n",
    "# some manual mapping to make names align\n",
    "mapper_dict = {\n",
    "    \"China, Hong Kong Special Administrative Region\": \"China, Hong Kong SAR\",\n",
    "    \"Côte d’Ivoire\": \"Côte d'Ivoire\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran, Islamic Republic of\",\n",
    "    \"Eswatini\": \"Kingdom of Eswatini\",\n",
    "    \"North Macedonia\": \"Republic of North Macedonia\",\n",
    "    \"Türkiye\": \"Turkey\",\n",
    "    \"United States of America\": \"United States\",\n",
    "    \"China\": \"China (mainland)\",\n",
    "}\n",
    "mapper_subregion = {\"South-eastern Asia\": \"South-Eastern Asia\"}\n",
    "un_regions[\"name\"] = un_regions.name.replace(mapper_dict)\n",
    "un_regions[\"subregion\"] = un_regions.subregion.replace(mapper_subregion)\n",
    "un_regions = un_regions.set_index(\"name\")\n",
    "\n",
    "# join datasets\n",
    "un_merged = un_y.join(un_regions, on=\"name\", how=\"left\")\n",
    "\n",
    "# check results\n",
    "mismatched = un_merged[\n",
    "    (un_merged.subregion.isnull()) & (un_merged.year >= sset.HISTORICAL_YEARS[0])\n",
    "].name.unique()\n",
    "ignore = [\n",
    "    \"Caribbean\",\n",
    "    \"Central America\",\n",
    "    \"China (mainland)\",\n",
    "    \"Eastern Africa\",\n",
    "    \"Middle Africa\",\n",
    "    \"South America\",\n",
    "    \"South-Eastern Asia\",\n",
    "    \"Southern Africa\",\n",
    "    \"Western Africa\",\n",
    "    \"World\",\n",
    "]\n",
    "region_names = np.concatenate(\n",
    "    (un_regions.region.unique(), un_regions.subregion.unique(), ignore)\n",
    ")\n",
    "assert np.isin(mismatched, region_names).all()\n",
    "\n",
    "# check that we don't have excess ccodes\n",
    "ccodes = un_merged.ccode.dropna().unique()\n",
    "assert np.isin(ccodes, valid_ccodes).all()\n",
    "\n",
    "# set index\n",
    "un_merged = un_merged.set_index([\"name\", \"year\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57b9673e-aca1-4daa-aa99-9874ae0d4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(un_regions, sset.PATH_UN_REGION_DATA_INT)\n",
    "save(un_merged, sset.PATH_UN_AMA_INT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
