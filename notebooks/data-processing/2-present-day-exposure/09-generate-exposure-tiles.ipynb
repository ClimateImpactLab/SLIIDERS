{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get elevation relative to sea level\n",
    "2. Match to nearest country, impact region, protection zone (e.g. levees)\n",
    "3. Uniformly distribute exposure over all surface area > 0 elevation within a 30\" pixel\n",
    "4. Aggregate both surface area and exposure up to region X segment X protection zone X wetland flag X .1-meter elevation bin\n",
    "5. Save area by elevation for each segment x wetland flag (for <0 elevations, only care about wetland area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regionmask\n",
    "import sliiders.settings as sset\n",
    "import xarray as xr\n",
    "from dask_gateway import GatewayCluster\n",
    "from shapely.geometry import box\n",
    "from sliiders import spatial\n",
    "from sliiders.dask import start_cluster\n",
    "from sliiders.io import open_rasterio, open_zarr, read_shapefile, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXELS_PER_TILE = 3601\n",
    "N_WORKERS = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb6bcd675ca4b1289c3e244286924b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>GatewayCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n<style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client, cluster = start_cluster()\n",
    "cluster.adapt(minimum=7, maximum=N_WORKERS)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_asset_value(llon, llat, ulon, ulat):\n",
    "    # Get corners of `bbox` by their indices\n",
    "    lx_ix, ux_ix = spatial.grid_val_to_ix(\n",
    "        np.array([llon, ulon]),\n",
    "        sset.ASSET_VALUE_GRID_WIDTH,\n",
    "    )\n",
    "\n",
    "    ly_ix, uy_ix = spatial.grid_val_to_ix(\n",
    "        np.array([llat, ulat]),\n",
    "        sset.ASSET_VALUE_GRID_WIDTH,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        pd.read_parquet(\n",
    "            sset.PATH_EXPOSURE_ASSET_VALUE_BLENDED,\n",
    "            columns=[\"value\", \"x_ix\", \"y_ix\"],\n",
    "            filters=[\n",
    "                [\n",
    "                    (\"x_ix\", \">=\", lx_ix),\n",
    "                    (\"x_ix\", \"<\", ux_ix),\n",
    "                    (\"y_ix\", \">=\", ly_ix),\n",
    "                    (\"y_ix\", \"<\", uy_ix),\n",
    "                ]\n",
    "            ],\n",
    "        )\n",
    "        .set_index([\"x_ix\", \"y_ix\"])\n",
    "        .value.rename(\"asset_value\")\n",
    "    )\n",
    "\n",
    "\n",
    "def open_pop(llon, llat, ulon, ulat):\n",
    "    lx_ix, ux_ix = spatial.grid_val_to_ix(\n",
    "        np.array([llon, ulon]),\n",
    "        sset.POP_GRID_WIDTH,\n",
    "    )\n",
    "\n",
    "    ly_ix, uy_ix = spatial.grid_val_to_ix(\n",
    "        np.array([llat, ulat]),\n",
    "        sset.POP_GRID_WIDTH,\n",
    "    )\n",
    "    return pd.read_parquet(\n",
    "        sset.PATH_EXPOSURE_POP_INT,\n",
    "        filters=[\n",
    "            [\n",
    "                (\"x_ix\", \">=\", lx_ix),\n",
    "                (\"x_ix\", \"<\", ux_ix),\n",
    "                (\"y_ix\", \">=\", ly_ix),\n",
    "                (\"y_ix\", \"<\", uy_ix),\n",
    "            ]\n",
    "        ],\n",
    "    ).population.rename(\"pop\")\n",
    "\n",
    "\n",
    "def load_exposure(llon, llat):\n",
    "    \"\"\"Get asset value and population within the bounds defined by `bbox`\"\"\"\n",
    "    return open_asset_value(llon, llat, llon + 1, llat + 1).astype(\"float64\"), open_pop(\n",
    "        llon, llat, llon + 1, llat + 1\n",
    "    ).astype(\"float64\")\n",
    "\n",
    "\n",
    "def convert_to_ser(mask, elev_tile):\n",
    "    mapper = {k: v.name for k, v in mask.regions.items()}\n",
    "    return (\n",
    "        mask.mask(elev_tile, wrap_lon=180)\n",
    "        .to_series()\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "        .replace(mapper)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_regions(bbox):\n",
    "    seg_adm = read_shapefile(\n",
    "        sset.PATH_SEG_REGION_VORONOI_INTERSECTIONS_SHP,\n",
    "        bbox=box(*bbox.buffer(0.1).bounds),\n",
    "        engine=\"pyogrio\",\n",
    "    )\n",
    "    return regionmask.from_geopandas(seg_adm, names=\"seg_adm\", name=\"seg_adm\")\n",
    "\n",
    "\n",
    "def _buffer_bbox(bbox, buffer_width=0.1):\n",
    "    buffered = box(*bbox.buffer(0.1).bounds)\n",
    "    return buffered.bounds\n",
    "\n",
    "\n",
    "def get_wetland_mangrove_areas(bbox, elev_tile):\n",
    "    \"\"\"\n",
    "    Get flag indicating existence of wetlands in `bbox`, returning a flattened array\n",
    "    corresponding to the flattened indices of `elev_tile`\n",
    "    \"\"\"\n",
    "    buff_bnds = _buffer_bbox(bbox)\n",
    "    mangroves = read_shapefile(\n",
    "        sset.PATH_GLOBAL_MANGROVES, bbox=buff_bnds, engine=\"pyogrio\"\n",
    "    )\n",
    "    wetlands = (\n",
    "        open_rasterio(sset.PATH_GLOBCOVER_2009)\n",
    "        .sel(y=slice(buff_bnds[3], buff_bnds[1]), x=slice(buff_bnds[0], buff_bnds[2]))\n",
    "        .squeeze(drop=True)\n",
    "        .isin([160, 170, 180])\n",
    "        .rename(y=\"lat\", x=\"lon\")\n",
    "        .astype(int)\n",
    "        .interp_like(elev_tile, method=\"nearest\")\n",
    "        .astype(bool)\n",
    "    )\n",
    "    if len(mangroves):\n",
    "        wetlands = (\n",
    "            wetlands\n",
    "            | regionmask.mask_geopandas(\n",
    "                mangroves.dissolve(\"PXLVAL\"),\n",
    "                wetlands,\n",
    "            ).notnull()\n",
    "        )\n",
    "    return wetlands\n",
    "\n",
    "\n",
    "def get_lake_mask(bbox, elev_tile):\n",
    "    buff_bnds = _buffer_bbox(bbox)\n",
    "    lake_region = gpd.read_parquet(sset.PATH_NATEARTH_LAKES_INT).clip_by_rect(\n",
    "        *buff_bnds\n",
    "    )\n",
    "    if lake_region.is_empty.all():\n",
    "        return False\n",
    "\n",
    "    lake_mask = regionmask.mask_geopandas(\n",
    "        lake_region,\n",
    "        elev_tile,\n",
    "    ).notnull()\n",
    "    return lake_mask\n",
    "\n",
    "\n",
    "def save_to_parquet(df, fpath):\n",
    "    dtypes = {\n",
    "        \"seg_adm\": \"category\",\n",
    "        \"protection_zone\": \"int64\",\n",
    "        \"z_ix\": \"int16\",\n",
    "        \"x_ix\": \"int16\",\n",
    "        \"y_ix\": \"int16\",\n",
    "        \"pop\": \"float64\",\n",
    "        \"area_km\": \"float64\",\n",
    "        \"asset_value\": \"float64\",\n",
    "        \"wetland_flag\": bool,\n",
    "    }\n",
    "    out = df.reset_index(drop=False).astype(\n",
    "        {k: v for k, v in dtypes.items() if k in df.columns}\n",
    "    )\n",
    "    assert out.columns.isin(dtypes.keys()).all(), [\n",
    "        c for c in out.columns if c not in dtypes.keys()\n",
    "    ]\n",
    "    save(out, fpath, index=False)\n",
    "\n",
    "\n",
    "def match_elev_pixels_to_shapes(elev_tile, bbox):\n",
    "    region_mask = get_regions(bbox)\n",
    "    protected_mask = get_protected_areas(bbox)\n",
    "    wetlands = get_wetland_mangrove_areas(bbox, elev_tile)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"seg_adm\": convert_to_ser(region_mask, elev_tile).astype(\"category\"),\n",
    "            \"protection_zone\": convert_to_ser(protected_mask, elev_tile),\n",
    "            \"wetland_flag\": wetlands.to_series(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def get_protected_areas(bbox):\n",
    "    protected = gpd.read_parquet(\n",
    "        sset.PATH_COMBINED_PROTECTED_AREAS,\n",
    "        columns=[\"geometry\"],\n",
    "    )\n",
    "    protected = protected[protected.geometry.overlaps(bbox)].reset_index()\n",
    "\n",
    "    # add in no-protection value\n",
    "    out = gpd.GeoDataFrame({\"protection_zone_id\": [-1]}, geometry=[bbox])\n",
    "\n",
    "    if len(protected):\n",
    "        out[\"geometry\"] = out.difference(protected.unary_union)\n",
    "        out = out[~out.is_empty]\n",
    "        out = pd.concat(\n",
    "            [\n",
    "                protected,\n",
    "                out,\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    return regionmask.from_geopandas(\n",
    "        out, names=\"protection_zone_id\", name=\"protection_zone_id\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _correct_vals(x, bins_per_int_per_side):\n",
    "    return np.arange(x - bins_per_int_per_side, x + bins_per_int_per_side + 1)\n",
    "\n",
    "\n",
    "def spread_int_res(df, high_elev_ix):\n",
    "    to_spread = df[df.index.get_level_values(\"int_res\")]\n",
    "    if not len(to_spread):\n",
    "        return df.droplevel(\"int_res\")\n",
    "\n",
    "    to_drop = to_spread.index.copy()\n",
    "    spread_vals = to_spread.index.get_level_values(\"z_ix\").unique()\n",
    "    names = [n for n in to_spread.index.names if n != \"int_res\"]\n",
    "\n",
    "    bins_per_int_per_side = round((1 / sset.EXPOSURE_BIN_WIDTH_V) / 2)\n",
    "\n",
    "    all_vals = spread_vals.map(\n",
    "        lambda x: _correct_vals(x, bins_per_int_per_side)\n",
    "    ).to_series()\n",
    "    all_vals.index = spread_vals\n",
    "    all_vals = all_vals.explode()\n",
    "    counts = all_vals.groupby(\"z_ix\").count().rename(\"counts\")\n",
    "\n",
    "    to_spread = (\n",
    "        to_spread.to_frame()\n",
    "        .join(all_vals.rename(\"new_z_ix\"), on=\"z_ix\", how=\"left\")\n",
    "        .join(counts, on=\"z_ix\", how=\"left\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    to_spread[\"z_ix\"] = to_spread.new_z_ix\n",
    "    to_spread[\"area_km\"] /= to_spread.counts\n",
    "\n",
    "    # drop the below MSL vals and clip at the high-elev threshold\n",
    "    to_spread[\"z_ix\"] = to_spread.z_ix.clip(upper=high_elev_ix)\n",
    "    to_spread = to_spread[(to_spread.z_ix >= 0) | (to_spread.protection_zone != -1)]\n",
    "\n",
    "    to_spread = to_spread.groupby(names, observed=True).area_km.sum()\n",
    "\n",
    "    return (\n",
    "        pd.concat((df.drop(to_drop).droplevel(\"int_res\"), to_spread))\n",
    "        .groupby(names, observed=True)\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "\n",
    "def process_tile(tile_name, calc_elev=True, calc_exp=True, check=True):\n",
    "    out_path = sset.DIR_EXPOSURE_BINNED_TMP_TILES / f\"{tile_name}.parquet\"\n",
    "    seg_area_out_path = (\n",
    "        sset.DIR_EXPOSURE_BINNED_TMP_TILES_SEGMENT_AREA / f\"{tile_name}.parquet\"\n",
    "    )\n",
    "    noland_path = sset.DIR_EXPOSURE_BINNED_TMP_TILES_NOLAND / f\"{tile_name}.parquet\"\n",
    "\n",
    "    if check and (\n",
    "        out_path.is_file()\n",
    "        or noland_path.is_file()\n",
    "        or (seg_area_out_path.is_file() and not calc_exp)\n",
    "    ):\n",
    "        return out_path\n",
    "\n",
    "    llon, llat = spatial.get_ll(tile_name)\n",
    "    lat_slice = slice((90 + llat) * PIXELS_PER_TILE, (91 + llat) * PIXELS_PER_TILE)\n",
    "    lon_slice = slice((180 + llon) * PIXELS_PER_TILE, (181 + llon) * PIXELS_PER_TILE)\n",
    "    bbox = box(llon, llat, llon + 1, llat + 1)\n",
    "\n",
    "    elev_tile = (\n",
    "        open_zarr(sset.PATH_ELEV_MSS, chunks=None)\n",
    "        .isel(lon=lon_slice, lat=lat_slice)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    connected = elev_tile.connected < 201\n",
    "    underwater = ((elev_tile.z < 0) | elev_tile.source.isin([1, 3])) & connected\n",
    "    int_res = (\n",
    "        elev_tile.int_res\n",
    "        & (elev_tile.z <= sset.HIGHEST_WITHELEV_EXPOSURE_METERS)\n",
    "        & connected\n",
    "    )\n",
    "    elev_tile = elev_tile.z\n",
    "\n",
    "    high_elev_fill = (\n",
    "        sset.HIGHEST_WITHELEV_EXPOSURE_METERS + sset.EXPOSURE_BIN_WIDTH_V / 2\n",
    "    )\n",
    "    high_elev_ix = spatial.grid_val_to_ix(\n",
    "        np.array(high_elev_fill), sset.EXPOSURE_BIN_WIDTH_V\n",
    "    )\n",
    "\n",
    "    if not connected.any():\n",
    "        elev_tile *= np.nan\n",
    "        calc_elev = False\n",
    "\n",
    "    if not calc_elev:\n",
    "        # mark it all as \"high-elev\"\n",
    "        assert elev_tile.isnull().all()\n",
    "        elev_tile = elev_tile.fillna(high_elev_fill)\n",
    "        underwater[:] = False\n",
    "    assert elev_tile.notnull().any()\n",
    "\n",
    "    # Bundle higher-than-coastal elevation values into one to simplify later data\n",
    "    # processing. Set non-hydro-connected pixels to the high-elev-fill value as well\n",
    "    # (treated same as high elevation)\n",
    "    elev_tile = elev_tile.where(\n",
    "        (elev_tile <= sset.HIGHEST_WITHELEV_EXPOSURE_METERS) & connected,\n",
    "        high_elev_fill,\n",
    "    )\n",
    "\n",
    "    # match tile points with countries, regions, protection zones\n",
    "    out = match_elev_pixels_to_shapes(elev_tile, bbox)\n",
    "\n",
    "    # get points on land, assign impact regions and countries at exposure grid level\n",
    "    elev_df = xr.Dataset(\n",
    "        {\n",
    "            \"elev\": elev_tile,\n",
    "            \"underwater\": underwater,\n",
    "            \"int_res\": int_res,\n",
    "            \"lake\": get_lake_mask(bbox, elev_tile),\n",
    "        }\n",
    "    ).to_dataframe()\n",
    "\n",
    "    out[\"z_ix\"] = pd.Series(\n",
    "        spatial.grid_val_to_ix(elev_df.elev, sset.EXPOSURE_BIN_WIDTH_V),\n",
    "        index=elev_df.index,\n",
    "    ).astype(\"int16\")\n",
    "\n",
    "    out[[\"underwater\", \"int_res\", \"elev\", \"lake\"]] = elev_df[\n",
    "        [\"underwater\", \"int_res\", \"elev\", \"lake\"]\n",
    "    ]\n",
    "\n",
    "    out[\"area_km\"] = (\n",
    "        np.cos(np.deg2rad(out.index.get_level_values(\"lat\")))\n",
    "        * (spatial.LAT_TO_M / 1000 / PIXELS_PER_TILE) ** 2\n",
    "    )\n",
    "\n",
    "    out = out.set_index(\n",
    "        [\n",
    "            pd.Index(\n",
    "                spatial.grid_val_to_ix(\n",
    "                    out.index.get_level_values(\"lon\"),\n",
    "                    sset.ASSET_VALUE_GRID_WIDTH,\n",
    "                    lon_mask=True,\n",
    "                ).astype(\"int16\"),\n",
    "                name=\"x_ix\",\n",
    "            ),\n",
    "            pd.Index(\n",
    "                spatial.grid_val_to_ix(\n",
    "                    out.index.get_level_values(\"lat\"),\n",
    "                    sset.ASSET_VALUE_GRID_WIDTH,\n",
    "                    lon_mask=True,\n",
    "                ).astype(\"int16\"),\n",
    "                name=\"y_ix\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    valid = ((out.z_ix.ge(0) & (~out.underwater)) | out.protection_zone.ne(-1)) & (\n",
    "        ~out.lake\n",
    "    )\n",
    "    out = out.drop(columns=[\"underwater\", \"lake\"])\n",
    "    negelev_wetland_pts = out[(~valid) & out.wetland_flag]\n",
    "    out = out[valid]\n",
    "\n",
    "    out = out.groupby(\n",
    "        [\n",
    "            \"seg_adm\",\n",
    "            \"protection_zone\",\n",
    "            \"wetland_flag\",\n",
    "            \"z_ix\",\n",
    "            \"x_ix\",\n",
    "            \"y_ix\",\n",
    "            \"int_res\",\n",
    "        ],\n",
    "        observed=True,\n",
    "    ).area_km.sum()\n",
    "\n",
    "    # spread out integer-resolution observations over the nearest 1m of bins\n",
    "    out = spread_int_res(out, high_elev_ix)\n",
    "\n",
    "    seg_areas = out.groupby(\n",
    "        [\"seg_adm\", \"protection_zone\", \"wetland_flag\", \"z_ix\"],\n",
    "        observed=True,\n",
    "    ).sum()\n",
    "\n",
    "    negelev_wetland_areas = negelev_wetland_pts.groupby(\n",
    "        [\"seg_adm\", \"protection_zone\", \"wetland_flag\", \"z_ix\"],\n",
    "        observed=True,\n",
    "    ).area_km.sum()\n",
    "\n",
    "    seg_areas = pd.concat([seg_areas, negelev_wetland_areas])\n",
    "    seg_areas = seg_areas[seg_areas.index.get_level_values(\"z_ix\") <= high_elev_ix]\n",
    "\n",
    "    if sset.ASSET_VALUE_GRID_WIDTH != sset.POP_GRID_WIDTH:\n",
    "        raise NotImplementedError(\n",
    "            \"A simple regridding is necessary to map to a different grid for \"\n",
    "            \"population, but has not yet been implemented\"\n",
    "        )\n",
    "\n",
    "    save_to_parquet(seg_areas.to_frame(), seg_area_out_path)\n",
    "\n",
    "    if not calc_exp:\n",
    "        return seg_area_out_path\n",
    "\n",
    "    assets, pop = load_exposure(llon, llat)\n",
    "    this_exp = assets.to_frame().join(pop, how=\"outer\").fillna(0)\n",
    "\n",
    "    if out.shape[0] == 0:\n",
    "        if calc_exp:\n",
    "            save_to_parquet(\n",
    "                this_exp,\n",
    "                noland_path,\n",
    "            )\n",
    "        return out_path\n",
    "\n",
    "    # swap these if we want to assume no population/capital on wetlands\n",
    "    # out = out[\n",
    "    #     ~out.index.get_level_values(\"wetland_flag\").values.astype(bool)\n",
    "    # ].droplevel(\"wetland_flag\")\n",
    "    out = out.groupby(\n",
    "        [c for c in out.index.names if c != \"wetland_flag\"], observed=True\n",
    "    ).sum()\n",
    "\n",
    "    this_exp[[\"lon\", \"lat\"]] = spatial.grid_ix_to_val(\n",
    "        this_exp.reset_index([\"x_ix\", \"y_ix\"])[[\"x_ix\", \"y_ix\"]].values,\n",
    "        sset.ASSET_VALUE_GRID_WIDTH,\n",
    "        lon_mask=[True, False],\n",
    "    )\n",
    "\n",
    "    valid_locs = this_exp.index.isin(\n",
    "        out.droplevel(\n",
    "            [c for c in out.index.names if c not in [\"x_ix\", \"y_ix\"]]\n",
    "        ).index.unique()\n",
    "    )\n",
    "\n",
    "    if not valid_locs.all():\n",
    "        to_merge = []\n",
    "        # ideally, we move this exposure that's has no valid x,y,z location within its\n",
    "        # grid cell to the nearest grid cell with a valid location AND existing exposure\n",
    "        if valid_locs.any():\n",
    "            valid_exp = this_exp[valid_locs]\n",
    "            to_merge.append(valid_exp)\n",
    "        # if none exist in this tile, simply move to the nearest grid cell w/ valid\n",
    "        # location, regardless of whether it contains exposure already.\n",
    "        else:\n",
    "            tmp = out.reset_index()[[\"x_ix\", \"y_ix\"]]\n",
    "            valid_exp = pd.DataFrame(\n",
    "                spatial.grid_ix_to_val(\n",
    "                    tmp.values, sset.ASSET_VALUE_GRID_WIDTH, lon_mask=[True, False]\n",
    "                ),\n",
    "                columns=[\"lon\", \"lat\"],\n",
    "                index=tmp.set_index([\"x_ix\", \"y_ix\"]).index,\n",
    "            )\n",
    "\n",
    "        extra_exp = this_exp[~valid_locs]\n",
    "        exp_ix_mappings = spatial.spherical_nearest_neighbor(extra_exp, valid_exp)\n",
    "        to_merge.append(\n",
    "            extra_exp.set_index(\n",
    "                pd.MultiIndex.from_tuples(\n",
    "                    exp_ix_mappings.values, names=extra_exp.index.names\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        this_exp = pd.concat(to_merge).groupby([\"x_ix\", \"y_ix\"]).sum()\n",
    "\n",
    "    this_exp = this_exp.drop(columns=[\"lon\", \"lat\"])\n",
    "\n",
    "    full = out.to_frame().join(this_exp, how=\"outer\")\n",
    "    assert full.area_km.notnull().all()\n",
    "    full[[\"asset_value\", \"pop\"]] = full[[\"asset_value\", \"pop\"]].fillna(0)\n",
    "\n",
    "    frac_areas = full.area_km / full.area_km.groupby([\"x_ix\", \"y_ix\"]).sum()\n",
    "    out = (\n",
    "        full[[\"asset_value\", \"pop\"]]\n",
    "        .mul(frac_areas, axis=\"index\")\n",
    "        .join(full.area_km, how=\"outer\")\n",
    "    )\n",
    "    assert out.notnull().all().all()\n",
    "\n",
    "    out = out.groupby(\n",
    "        [c for c in out.index.names if c not in [\"x_ix\", \"y_ix\"]], observed=True\n",
    "    ).sum()\n",
    "\n",
    "    # make sure no exposure was dropped or added from the original exposure within tile\n",
    "    # (within some margin of float error)\n",
    "    # include very low sums for 0 / 0 division (areas where there is no exposure, but we\n",
    "    # calculate anyway for diva areas)\n",
    "    assert (\n",
    "        this_exp.sum().sum() < 0.00001\n",
    "        or np.abs(this_exp.sum() / out[this_exp.columns].sum() - 1).sum() < 0.00001\n",
    "    )\n",
    "\n",
    "    # make sure range is acceptable\n",
    "    z = out.index.get_level_values(\"z_ix\")[\n",
    "        out.index.get_level_values(\"protection_zone\") == -1\n",
    "    ]\n",
    "\n",
    "    assert (len(z) == 0) or (z.min() >= 0) & (z.max() <= high_elev_ix)\n",
    "\n",
    "    save_to_parquet(out, out_path)\n",
    "\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of tiles to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_meta = pd.read_parquet(sset.PATH_EXPOSURE_TILE_LIST)\n",
    "\n",
    "tile_groups = (\n",
    "    tile_meta.reset_index().groupby(\"PROCESSING_SET\")[\"tile_name\"].unique().to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Without elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutelev_futures = client.map(\n",
    "    process_tile, tile_groups[\"WITHOUTELEV\"], calc_elev=False, batch_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## With elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "withelev_futures = client.map(\n",
    "    process_tile,\n",
    "    tile_groups[\"WITHELEV\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No exposure (just area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciam_futures = client.map(\n",
    "    process_tile,\n",
    "    tile_groups[\"CIAM\"],\n",
    "    calc_exp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check unassigned exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finished = False\n",
    "client.gather(withoutelev_futures + withelev_futures + ciam_futures)\n",
    "finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asset_value    138.70167\n",
      "pop            605.00000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_value</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N09E081</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N74E115</th>\n",
       "      <td>73.054528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N74E116</th>\n",
       "      <td>63.280207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N80E078</th>\n",
       "      <td>2.366936</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S09E162</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>603.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asset_value    pop\n",
       "N09E081     0.000000    2.0\n",
       "N74E115    73.054528    0.0\n",
       "N74E116    63.280207    0.0\n",
       "N80E078     2.366936    0.0\n",
       "S09E162     0.000000  603.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noland = [\n",
    "    i.split(\"/\")[-1] for i in sset.FS.ls(str(sset.DIR_EXPOSURE_BINNED_TMP_TILES_NOLAND))\n",
    "]\n",
    "noland = pd.concat(\n",
    "    [\n",
    "        pd.read_parquet(\n",
    "            sset.DIR_EXPOSURE_BINNED_TMP_TILES_NOLAND / t,\n",
    "            columns=[\"asset_value\", \"pop\"],\n",
    "        )\n",
    "        .sum()\n",
    "        .to_frame()\n",
    "        .T.set_index(pd.Index([t.split(\".\")[0]]))\n",
    "        for t in noland\n",
    "    ]\n",
    ")\n",
    "assert (noland.sum() < 700).all()\n",
    "print(noland.sum())\n",
    "noland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is less than 140 people and around $600 in assets that failed to be assigned a location because they were in a 1-degree tile with no valued x/y/z locations. These are small enough values on the global scale and well within the uncertainty even for a single grid cell, so we drop these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
